# Sparkify Analytics Database & ETL

## Objective

Provide a functional database with songs users are listening to,
so the analytics team can work with.

## Datasets

The data used in the ETL pipeline is from the original files 
(in JSON), generated by the music streaming app. There are two
datasets, described bellow:

- Songs Dataset: Contains metadata about a song and the artist
of that song.

```json
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

- Log Dataset: Activity from the music app. 

## Running 

To run the project you are going to need Pipenv. Pipenv is used 
so you don't need to install the dependencies locally. 

```commandline
$ pipenv run python <file.py>
```

## Starting

There are two main scripts to run in order to create the database
and importing the data. 

The first one, "create_tables.py", will
drop the tables if they exists and re-creating them. 

```commandline
$ pipenv run python create_tables.py
```

The second one, "etl.py", will load song and log data from the 
datasets located on "data" folder and insert this date properly 
on the tables previously created.

```commandline
$ pipenv run python etl.py
```

Both scripts use "sql_queries.py" in order to achieve the result.
There are some auxiliary .ipynb files that were used to create
the tables. 

## Docker Enviroment 

Run PySpark docker image  w/ and name (pyspark_dend)

```commandline
$ docker run \
    --name pyspark_dend \
    -d \
    -p 8888:8888\
    -v <folder>:/home/jovyan/
    jupyter/pyspark-notebook
```

Stop the container:

```commandline
$ docker ps
$ docker kill <container_id>
```

Start the container:
```commandline
$ docker start pyspark_dend
```

Remove container (remove all data, while stop just stop running)

```commandline
$ docker ps -a
$ docker rm <container_id>
```

To use container notebook local, enter on http://localhost:8888/tree
If asks for the token execute the following command and get the
return url and put on browser.

```commandline
$ docker exec pyspark_dend jupyter notebook list
```

To run python script with container spark submit, use:

```commandline
$ docker exec pyspark_dend python3 <script>.py
```

## References

- http://ondata.blog/articles/getting-started-apache-spark-pyspark-and-jupyter-in-a-docker-container/