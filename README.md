# Sparkify Analytics Datalake

## Objective

Provide a functional Data Lake on AWS S3 with songs users are 
listening to. 

## Datasets

The data used in the ETL pipeline is from the original files 
(in JSON), generated by the music streaming app. There are two
datasets, described bellow:

- Songs Dataset: Contains metadata about a song and the artist
of that song.

```json
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

- Log Dataset: Activity from the music app. 

## Files

The dl.example.cfg file is the AWS configuration for the project.

The script etl.py executes the steps necessary to read the S3
data and create the datalake files and folder.

ETL.ipynb describe in further details the steps used to create
the datalake. 

## Running 

To run the project you are going to need PySpark or a docker 
environment, docker configuration describe bellow.

To run the script just use (inside docker container).

```commandline
$ python3 <file.py>
```

## Docker Enviroment 

Run PySpark docker image  w/ and name (pyspark_dend)

```commandline
$ docker run \
    --name pyspark_dend \
    -d \
    -p 8888:8888\
    -v <folder>:/home/jovyan/
    jupyter/pyspark-notebook
```

Stop the container:

```commandline
$ docker ps
$ docker kill <container_id>
```

Start the container:
```commandline
$ docker start pyspark_dend
```

Remove container (remove all data, while stop just stop running)

```commandline
$ docker ps -a
$ docker rm <container_id>
```

To use container notebook local, enter on http://localhost:8888/tree
If asks for the token execute the following command and get the
return url and put on browser.

```commandline
$ docker exec pyspark_dend jupyter notebook list
```

To run python script with container spark submit, use:

```commandline
$ docker exec pyspark_dend python3 <script>.py
```

## Execute in production, using AWS EMR

Use AWS EMR to execute the jobs in production.

After creating a cluster send the script and cfg file to the 
EMR Cluster using scp

```commandline
$ scp -i <cluster key> <script> <cluster user>@<cluster dns>:~/
$ scp -i \~/Documents/pessoal/projetos.nosync/rlsr.pem etl.py hadoop@ec2-3-224-147-15.compute-1.amazonaws.com:~/
```
Connect to the cluster via ssh and execute spark-submit.

```commandline
$ ssh -i <cluster key> <cluster user>@<cluster dns>
```

Inside the Cluster find spark-submit and run the script.

```commandline
$ which spark-submit 
$ apache-submit --master yarn <script.py>
```

You may have to change python version the EMR Cluster is using.
Set the environment variable for that.

```commandline
$ which python3
$ export PYSPARK_PYTHON=/usr/bin/python3
```

## References

- [http://ondata.blog/articles/getting-started-apache-spark-pyspark-and-jupyter-in-a-docker-container/]